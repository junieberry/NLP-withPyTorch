{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_SequenceModeling_forIntermediate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMBbnLhtow1r7mAkbFCXvGw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junieberry/NLP-withPyTorch/blob/main/07_SequenceModeling_forIntermediate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMVqgEBjtpSQ"
      },
      "source": [
        "이번 장의 목표는 **시퀀스 예측!** 혹은 시퀀스 레이블링\n",
        "\n",
        "- 자연어 모델링 : 각 타임 스텝에서 주어진 단어 시퀀스를 기반으로 다음 단어를 예측\n",
        "- 품사 태깅 : 단어의 문법 품사를 예측\n",
        "- 개체명 인식 : 단어를 사람, 위치, 제품, 회사 같은 개체명으로 구분\n",
        "\n",
        "엘만 순환 신경망의 단점 >> 멀리 떨어진 의존성을 잘 감지하지 못한다.\n",
        "\n",
        "따라서 **gate network**라는 새로운 RNN 구조를 배운다!\n",
        "\n",
        "또한 **natural language generation**과 출력 시퀀스를 특정 방식으로 제한하는 조건부 생성을 공부한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCbrS-Qtuh44"
      },
      "source": [
        "## 7.1 엘만 RNN의 문제점\n",
        "\n",
        "**엘만 RNN의 문제**\n",
        "\n",
        "1. 멀리 떨어진 문제를 예측에 활용하지 못한다.\n",
        "\n",
        "  타임 스텝마다 정보의 유익성에 상관 없이 은닉 벡터를 업데이트한다.\n",
        "\n",
        "  즉, 별로 의미 없는 정보도 없데이트가 된다.\n",
        "\n",
        "  따라서 **RNN이 선택적으로 업데이트를 하거나 은닉 벡터의 일정 부분만을 업데이트해야한다**.\n",
        "\n",
        "2. 그레디언트가 불안정하다.\n",
        "\n",
        "  그레디언트 소실 혹은 그레디언트 폭발이 발생한다.\n",
        "\n",
        "  ReLU 사용이나 그레이언트 클리핑, 적절한 가중치 초기화 등이 있지만.. 무엇보다 **Gating**이 좋다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_0U5tkDvSUJ"
      },
      "source": [
        "## 7.2 엘만 RNN의 문제 해결책 : 게이팅\n",
        "\n",
        "<br>\n",
        "\n",
        "**게이트의 기본 개념**\n",
        "\n",
        "게이트는 정보의 흐름을 조절한다.\n",
        "\n",
        "$a + kb$\n",
        "\n",
        "\n",
        "k가 0에서 1 사이의 값일 때, k = 0이면 b의 기여는 없고, k = 1이면 b는 최대로 기여하게 된다.\n",
        "\n",
        "따라서 k를 사용해 b의 흐름도를 제어한다.\n",
        "\n",
        "<br>\n",
        "\n",
        "**게이팅을 활용한 엘만 RNN 업데이트 식**\n",
        "\n",
        "$h_t = h_{t-1} + F(h_{t-1}, x_t)$\n",
        "\n",
        "이때 `F()`는 RNN의 순환 계산으로, 0과 1 사이를 출력하는 함수이다. 이는 *조건 없는* 덧셈이다.\n",
        "\n",
        "<br>\n",
        "\n",
        "$h_t = h_{t-1} + λ(h_{t-1}, x_t)F(h_{t-1}, x_t)$\n",
        "\n",
        "`λ()`는 문맥에 따라 달라지며, 이전 상태를 업데이트하는데 현재 입력이 얼마나 들어가는지를 제어하는 함수이다.\n",
        "\n",
        "<br>\n",
        "\n",
        "**LSTM**\n",
        "\n",
        "LSTM 신경망은 이러한 게이팅 아이디어를 기반으로 조건에 따라 업데이트하는 것뿐만 아니라 이전 은닉 상태의 값을 의도적으로 지운다.\n",
        "\n",
        "$h_t = μ(h_{t-1}, x_t)h_{t-1} + λ(h_{t-1}, x_t)F(h_{t-1}, x_t)$\n",
        "\n",
        "\n",
        "이거 외에도 **GRU**가 있다!\n",
        "\n",
        "아무튼 게이팅은 업데이트 과정을 제어하고 그레디언트 이슈를 억제하고 훈련을 쉽게 해준다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_irAxCYjot_b"
      },
      "source": [
        "## 7.3 문자 RNN으로 성씨 생성하기\n",
        "\n",
        "[코드 구경하러 가기](https://github.com/junieberry/NLP-withPyTorch/blob/main/07_GRU_Surname/07_GRU_Surname.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A7QTIpfpBMt"
      },
      "source": [
        "## 7.4 시퀀스 모델 훈련 노하우\n",
        "\n",
        "1. 게이트가 있는 셀 사용하기\n",
        "2. LSTM보다는 GRU 사용하기\n",
        "3. 그레디언트 클리핑 사용하기\n",
        "4. Ealry Stopping 사용하기"
      ]
    }
  ]
}