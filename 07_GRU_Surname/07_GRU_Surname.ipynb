{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_GRU_Surname.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO1iS0WXbeAM8ND0bSrgQuT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junieberry/NLP-withPyTorch/blob/main/07_GRU_Surname/07_GRU_Surname.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peXjMe0m9lMY"
      },
      "source": [
        "import os\n",
        "from argparse import Namespace\n",
        "from collections import Counter\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tqdm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCmljpPq7OL5"
      },
      "source": [
        "# 7.3 문자 RNN으로 성씨 생성하기\n",
        "\n",
        "각 타임 스텝에서 RNN이 성씨에 포함될 수 있는 문자 집합에 대한 확률 분포를 계산\n",
        "\n",
        "\n",
        "1. 조건이 없는 SurnamgeGenerationModel\n",
        "\n",
        "  국적 정보를 사용하지 않고 성씨 문자의 시퀀스를 예측\n",
        "\n",
        "2. 조건이 있는 SurnameGenerationModel\n",
        "\n",
        "  초기 은닉 상태에 임베딩된 특정 국적을 활용해서 시퀀스 예\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuDjd79X9lvw"
      },
      "source": [
        "### 7.3.1 SurnamgeDataset 클래스\n",
        "\n",
        "- 판다스 데이터프레임을 사용해 데이터셋과 SurnameDataset 객체를 로드\n",
        "- SurnamgeVectorizer은 토큰-정수 매핑\n",
        "- 이때 `SurnamgeDatase.__getitem__()`메서드는 예측 타깃에 대한 정수 시퀀스 출력\n",
        "- 이 메서드는 `from_vector`과 `to_vector` 계산"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZqQGQMD9koa"
      },
      "source": [
        "\n",
        "class SurnameDataset(Dataset):\n",
        "    def __init__(self, surname_df, vectorizer):\n",
        "        \"\"\"\n",
        "        매개변수:\n",
        "            surname_df (pandas.DataFrame): 데이터셋\n",
        "            vectorizer (SurnameVectorizer): 데이터셋에서 만든 Vectorizer 객체\n",
        "        \"\"\"\n",
        "        self.surname_df = surname_df \n",
        "        self._vectorizer = vectorizer\n",
        "\n",
        "        self._max_seq_length = max(map(len, self.surname_df.surname)) + 2\n",
        "\n",
        "        self.train_df = self.surname_df[self.surname_df.split=='train']\n",
        "        self.train_size = len(self.train_df)\n",
        "\n",
        "        self.val_df = self.surname_df[self.surname_df.split=='val']\n",
        "        self.validation_size = len(self.val_df)\n",
        "\n",
        "        self.test_df = self.surname_df[self.surname_df.split=='test']\n",
        "        self.test_size = len(self.test_df)\n",
        "\n",
        "        self._lookup_dict = {'train': (self.train_df, self.train_size), \n",
        "                             'val': (self.val_df, self.validation_size), \n",
        "                             'test': (self.test_df, self.test_size)}\n",
        "\n",
        "        self.set_split('train')\n",
        "        \n",
        "    @classmethod\n",
        "    def load_dataset_and_make_vectorizer(cls, surname_csv):\n",
        "        \"\"\"데이터셋을 로드하고 새로운 Vectorizer를 만듭니다\n",
        "        \n",
        "        매개변수:\n",
        "            surname_csv (str): 데이터셋의 위치\n",
        "        반환값:\n",
        "            SurnameDataset 객체\n",
        "        \"\"\"        \n",
        "        surname_df = pd.read_csv(surname_csv)\n",
        "        return cls(surname_df, SurnameVectorizer.from_dataframe(surname_df))\n",
        "        \n",
        "    @classmethod\n",
        "    def load_dataset_and_load_vectorizer(cls, surname_csv, vectorizer_filepath):\n",
        "        \"\"\"데이터셋과 새로운 Vectorizer 객체를 로드합니다.\n",
        "        캐시된 Vectorizer 객체를 재사용할 때 사용합니다.\n",
        "        \n",
        "        매개변수:\n",
        "            surname_csv (str): 데이터셋의 위치\n",
        "            vectorizer_filepath (str): Vectorizer 객체의 저장 위치\n",
        "        반환값:\n",
        "            SurnameDataset의 인스턴스\n",
        "        \"\"\"\n",
        "        surname_df = pd.read_csv(surname_csv)\n",
        "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
        "        return cls(surname_df, vectorizer)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_vectorizer_only(vectorizer_filepath):\n",
        "        \"\"\"파일에서 Vectorizer 객체를 로드하는 정적 메서드\n",
        "        \n",
        "        매개변수:\n",
        "            vectorizer_filepath (str): 직렬화된 Vectorizer 객체의 위치\n",
        "        반환값:\n",
        "            SurnameVectorizer의 인스턴스\n",
        "        \"\"\"\n",
        "        with open(vectorizer_filepath) as fp:\n",
        "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
        "\n",
        "    def save_vectorizer(self, vectorizer_filepath):\n",
        "        \"\"\"Vectorizer 객체를 json 형태로 디스크에 저장합니다\n",
        "        \n",
        "        매개변수:\n",
        "            vectorizer_filepath (str): Vectorizer 객체의 저장 위치\n",
        "        \"\"\"\n",
        "        with open(vectorizer_filepath, \"w\") as fp:\n",
        "            json.dump(self._vectorizer.to_serializable(), fp)\n",
        "\n",
        "    def get_vectorizer(self):\n",
        "        \"\"\" 벡터 변환 객체를 반환합니다 \"\"\"\n",
        "        return self._vectorizer\n",
        "\n",
        "    def set_split(self, split=\"train\"):\n",
        "        self._target_split = split\n",
        "        self._target_df, self._target_size = self._lookup_dict[split]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._target_size\n",
        "\n",
        "     ##################################################################\n",
        "\n",
        "     ## 예측 타깃에 대한 정수 시퀀스 출력\n",
        "     ## 예측 타깃에 대한 정수 시퀀스 출력\n",
        "     ## vectorize 메서드로 입력으로 사용되는 정수 시퀀스와 출력으로 사용되는 정수 시퀀스를 계산\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"파이토치 데이터셋의 주요 진입 메서드\n",
        "        \n",
        "        매개변수:\n",
        "            index (int): 데이터 포인트에 대한 인덱스 \n",
        "        반환값:\n",
        "            데이터 포인트(x_data, y_target, class_index)를 담고 있는 딕셔너리\n",
        "        \"\"\"\n",
        "        row = self._target_df.iloc[index]\n",
        "        \n",
        "        from_vector, to_vector = \\\n",
        "            self._vectorizer.vectorize(row.surname, self._max_seq_length)\n",
        "        \n",
        "        nationality_index = \\\n",
        "            self._vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
        "\n",
        "        return {'x_data': from_vector, \n",
        "                'y_target': to_vector, \n",
        "                'class_index': nationality_index}\n",
        "        \n",
        "        \n",
        "     ##################################################################\n",
        "\n",
        "    def get_num_batches(self, batch_size):\n",
        "        \"\"\"배치 크기가 주어지면 데이터셋으로 만들 수 있는 배치 개수를 반환합니다\n",
        "        \n",
        "        매개변수:\n",
        "            batch_size (int)\n",
        "        반환값:\n",
        "            배치 개수\n",
        "        \"\"\"\n",
        "        return len(self) // batch_size\n",
        "    \n",
        "def generate_batches(dataset, batch_size, shuffle=True,\n",
        "                     drop_last=True, device=\"cpu\"): \n",
        "    \"\"\"\n",
        "    파이토치 DataLoader를 감싸고 있는 제너레이터 함수.\n",
        "    걱 텐서를 지정된 장치로 이동합니다.\n",
        "    \"\"\"\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
        "                            shuffle=shuffle, drop_last=drop_last)\n",
        "\n",
        "    for data_dict in dataloader:\n",
        "        out_data_dict = {}\n",
        "        for name, tensor in data_dict.items():\n",
        "            out_data_dict[name] = data_dict[name].to(device)\n",
        "        yield out_data_dict"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPKIvNFhDIzM"
      },
      "source": [
        "### 7.3.2 벡터 변환 클래스\n",
        "\n",
        "- SequenceVocabulary >> 개별 토큰 정수로 매핑\n",
        "- SurnameVectorizer >> 정수 매핑 관리\n",
        "- DataLoader >> SurnameVectorizer의 결과를 미니배치로\n",
        "\n",
        "**SurnameVectorizer와 END-OF-Sequence**\n",
        "\n",
        "\n",
        "타임 스텝마다 토큰 샘플과 토큰 타깃을 표현하는 정수 시퀀스 2개를 기대\n",
        "\n",
        "```\n",
        "예제의 성씨와 같이 훈련 시퀀스가 예측 대상이 됨\n",
        "\n",
        "하나의 토큰 시퀀스에서 토큰을 하나씩 엇갈리게 하는 식으로 샘플과 타깃 구성.. 진짜 무슨 말이지\n",
        "\n",
        "아 이해햇음\n",
        "\n",
        "0 1 2 3 끝\n",
        "\n",
        "시작 0 1 2 3\n",
        "\n",
        "요렇게\n",
        "```\n",
        "1. SequenceVocabulary로 각 토큰을 적절한 인덱스로 매핑\n",
        "2. 시퀀스 시작과 시퀀스 끝에 `BEGIN-OF-SEQUNCE`와 `END-OF-SEQUENCE` 토큰 추가\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9xdN2pGQkei"
      },
      "source": [
        "class Vocabulary(object):\n",
        "    \"\"\"매핑을 위해 텍스트를 처리하고 어휘 사전을 만드는 클래스 \"\"\"\n",
        "\n",
        "    def __init__(self, token_to_idx=None):\n",
        "        \"\"\"\n",
        "        매개변수:\n",
        "            token_to_idx (dict): 기존 토큰-인덱스 매핑 딕셔너리\n",
        "        \"\"\"\n",
        "\n",
        "        if token_to_idx is None:\n",
        "            token_to_idx = {}\n",
        "        self._token_to_idx = token_to_idx\n",
        "\n",
        "        self._idx_to_token = {idx: token \n",
        "                              for token, idx in self._token_to_idx.items()}\n",
        "        \n",
        "    def to_serializable(self):\n",
        "        \"\"\" 직렬화할 수 있는 딕셔너리를 반환합니다 \"\"\"\n",
        "        return {'token_to_idx': self._token_to_idx}\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        \"\"\" 직렬화된 딕셔너리에서 Vocabulary 객체를 만듭니다 \"\"\"\n",
        "        return cls(**contents)\n",
        "\n",
        "    def add_token(self, token):\n",
        "        \"\"\" 토큰을 기반으로 매핑 딕셔너리를 업데이트합니다\n",
        "\n",
        "        매개변수:\n",
        "            token (str): Vocabulary에 추가할 토큰\n",
        "        반환값:\n",
        "            index (int): 토큰에 상응하는 정수\n",
        "        \"\"\"\n",
        "        if token in self._token_to_idx:\n",
        "            index = self._token_to_idx[token]\n",
        "        else:\n",
        "            index = len(self._token_to_idx)\n",
        "            self._token_to_idx[token] = index\n",
        "            self._idx_to_token[index] = token\n",
        "        return index\n",
        "            \n",
        "    def add_many(self, tokens):\n",
        "        \"\"\"토큰 리스트를 Vocabulary에 추가합니다.\n",
        "        \n",
        "        매개변수:\n",
        "            tokens (list): 문자열 토큰 리스트\n",
        "        반환값:\n",
        "            indices (list): 토큰 리스트에 상응되는 인덱스 리스트\n",
        "        \"\"\"\n",
        "        return [self.add_token(token) for token in tokens]\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        \"\"\"토큰에 대응하는 인덱스를 추출합니다.\n",
        "        \n",
        "        매개변수:\n",
        "            token (str): 찾을 토큰 \n",
        "        반환값:\n",
        "            index (int): 토큰에 해당하는 인덱스\n",
        "        \"\"\"\n",
        "        return self._token_to_idx[token]\n",
        "\n",
        "    def lookup_index(self, index):\n",
        "        \"\"\" 인덱스에 해당하는 토큰을 반환합니다.\n",
        "        \n",
        "        매개변수: \n",
        "            index (int): 찾을 인덱스\n",
        "        반환값:\n",
        "            token (str): 인텍스에 해당하는 토큰\n",
        "        에러:\n",
        "            KeyError: 인덱스가 Vocabulary에 없을 때 발생합니다.\n",
        "        \"\"\"\n",
        "        if index not in self._idx_to_token:\n",
        "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
        "        return self._idx_to_token[index]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._token_to_idx)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1jGLR-sQmd8"
      },
      "source": [
        "class SequenceVocabulary(Vocabulary):\n",
        "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
        "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
        "                 end_seq_token=\"<END>\"):\n",
        "\n",
        "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
        "\n",
        "        self._mask_token = mask_token\n",
        "        self._unk_token = unk_token\n",
        "        self._begin_seq_token = begin_seq_token\n",
        "        self._end_seq_token = end_seq_token\n",
        "\n",
        "        self.mask_index = self.add_token(self._mask_token)\n",
        "        self.unk_index = self.add_token(self._unk_token)\n",
        "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
        "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
        "\n",
        "    def to_serializable(self):\n",
        "        contents = super(SequenceVocabulary, self).to_serializable()\n",
        "        contents.update({'unk_token': self._unk_token,\n",
        "                         'mask_token': self._mask_token,\n",
        "                         'begin_seq_token': self._begin_seq_token,\n",
        "                         'end_seq_token': self._end_seq_token})\n",
        "        return contents\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        \"\"\" 토큰에 대응하는 인덱스를 추출합니다.\n",
        "        토큰이 없으면 UNK 인덱스를 반환합니다.\n",
        "        \n",
        "        매개변수:\n",
        "            token (str): 찾을 토큰 \n",
        "        반환값:\n",
        "            index (int): 토큰에 해당하는 인덱스\n",
        "        노트:\n",
        "            UNK 토큰을 사용하려면 (Vocabulary에 추가하기 위해)\n",
        "            `unk_index`가 0보다 커야 합니다.\n",
        "        \"\"\"\n",
        "        if self.unk_index >= 0:\n",
        "            return self._token_to_idx.get(token, self.unk_index)\n",
        "        else:\n",
        "            return self._token_to_idx[token]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XlnrDilQurR"
      },
      "source": [
        "\n",
        "class SurnameVectorizer(object):\n",
        "    \"\"\" 어휘 사전을 생성하고 관리합니다 \"\"\"\n",
        "    def __init__(self, char_vocab, nationality_vocab):\n",
        "        \"\"\"\n",
        "        매개변수:\n",
        "            char_vocab (Vocabulary): 문자를 정수로 매핑합니다\n",
        "            nationality_vocab (Vocabulary): 국적을 정수로 매핑합니다\n",
        "        \"\"\"\n",
        "        self.char_vocab = char_vocab\n",
        "        self.nationality_vocab = nationality_vocab\n",
        "\n",
        "##############################################################################################\n",
        "\n",
        "    def vectorize(self, surname, vector_length=-1):\n",
        "        \"\"\" 성씨를 샘플과 타깃 벡터로 변환합니다\n",
        "        성씨 벡터를 두 개의 벡터 surname[:-1]와 surname[1:]로 나누어 출력합니다.\n",
        "        각 타임스텝에서 첫 번째 벡터가 샘플이고 두 번째 벡터가 타깃입니다.\n",
        "        \n",
        "        매개변수:\n",
        "            surname (str): 벡터로 변경할 성씨\n",
        "            vector_length (int): 인덱스 벡터의 길이를 맞추기 위한 매개변수\n",
        "        반환값:\n",
        "            튜플: (from_vector, to_vector)\n",
        "                from_vector (numpy.ndarray): 샘플 벡터 \n",
        "                to_vector (numpy.ndarray): 타깃 벡터 vector\n",
        "        \"\"\"\n",
        "\n",
        "        ## 성씨 이름을 정수로 매핑하고 앞 뒤에 토큰을 더해줌\n",
        "        indices = [self.char_vocab.begin_seq_index] \n",
        "        indices.extend(self.char_vocab.lookup_token(token) for token in surname)\n",
        "        indices.append(self.char_vocab.end_seq_index)\n",
        "\n",
        "        if vector_length < 0:\n",
        "            vector_length = len(indices) - 1\n",
        "\n",
        "        ## from_vector 만들어주기\n",
        "        from_vector = np.empty(vector_length, dtype=np.int64)         \n",
        "        from_indices = indices[:-1]\n",
        "        from_vector[:len(from_indices)] = from_indices\n",
        "        # mask index로 패딩\n",
        "        from_vector[len(from_indices):] = self.char_vocab.mask_index\n",
        "\n",
        "        ## to_vector 만들어주기\n",
        "        to_vector = np.empty(vector_length, dtype=np.int64)\n",
        "        to_indices = indices[1:]\n",
        "        to_vector[:len(to_indices)] = to_indices\n",
        "        #mask index로 패딩\n",
        "        to_vector[len(to_indices):] = self.char_vocab.mask_index\n",
        "        \n",
        "        return from_vector, to_vector\n",
        "\n",
        "\n",
        "##############################################################################################\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def from_dataframe(cls, surname_df):\n",
        "        \"\"\"데이터셋 데이터프레임으로 객체를 초기화합니다\n",
        "        \n",
        "        매개변수:\n",
        "            surname_df (pandas.DataFrame): 성씨 데이터셋\n",
        "        반환값:\n",
        "            SurnameVectorizer 객체\n",
        "        \"\"\"\n",
        "        char_vocab = SequenceVocabulary()\n",
        "        nationality_vocab = Vocabulary()\n",
        "\n",
        "        for index, row in surname_df.iterrows():\n",
        "            for char in row.surname:\n",
        "                char_vocab.add_token(char)\n",
        "            nationality_vocab.add_token(row.nationality)\n",
        "\n",
        "        return cls(char_vocab, nationality_vocab)\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        \"\"\"파일에서 SurnameVectorizer 객체를 초기화합니다\n",
        "        \n",
        "        매개변수:\n",
        "            contents (dict): SurnameVectorizer를 위해 두 개의 어휘 사전을 담은 딕셔너리\n",
        "                이 딕셔너리는 `vectorizer.to_serializable()`를 사용해 만듭니다\n",
        "        반환값:\n",
        "            SurnameVectorizer의 객체\n",
        "        \"\"\"\n",
        "        char_vocab = SequenceVocabulary.from_serializable(contents['char_vocab'])\n",
        "        nat_vocab =  Vocabulary.from_serializable(contents['nationality_vocab'])\n",
        "\n",
        "        return cls(char_vocab=char_vocab, nationality_vocab=nat_vocab)\n",
        "\n",
        "    def to_serializable(self):\n",
        "        \"\"\" 직렬화된 결과를 반환합니다 \"\"\"\n",
        "        return {'char_vocab': self.char_vocab.to_serializable(), \n",
        "                'nationality_vocab': self.nationality_vocab.to_serializable()}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22Dp6b6ZDVza"
      },
      "source": [
        "### 7.3.3 ElmanRNN을 GRU로 바꾸기\n",
        "\n",
        "LSTM이나 GRU로 바꾸는게 쉽다고 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sslb8spyRvu4"
      },
      "source": [
        "### 7.3.4 모델 1 : 조건이 없는 SurnameGenerationModel\n",
        "\n",
        "성씨를 생성하기 전에 국적 정보를 사용하지 않는다.\n",
        "\n",
        "따라서 GRU가 편향되지 않은 계산을 수행\n",
        "\n",
        "**SurnameGenerationModel**\n",
        "\n",
        "- Embedding층, GRU층, Linear층 초기화\n",
        "- 정수를 3차원 텐서로 변환\n",
        "- 문자 인덱스 임베딩하여 GRU로 상태 계산\n",
        "- Linear 층을 사용해 토큰의 예측 확률 계산\n",
        "\n",
        "\n",
        "6장과의 가장 큰 차이는 상태 벡터의 처리다!\n",
        "- 6장에서는 배치 인덱스마다 하나의 벡터를 얻고 이를 기반으로 예측\n",
        "- 7장에서는 3차원 텐서를 2차원 텐서로 바꿔서 예측 벡터 계산"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMUHkw9X61IM"
      },
      "source": [
        "class SurnameGenerationModel(nn.Module):\n",
        "    def __init__(self, char_embedding_size, char_vocab_size, rnn_hidden_size, \n",
        "                 batch_first=True, padding_idx=0, dropout_p=0.5):\n",
        "        \"\"\"\n",
        "        매개변수:\n",
        "            char_embedding_size (int): 문자 임베딩 크기\n",
        "            char_vocab_size (int): 임베딩될 문자 개수\n",
        "            rnn_hidden_size (int): RNN의 은닉 상태 크기\n",
        "            batch_first (bool): 0번째 차원이 배치인지 시퀀스인지 나타내는 플래그\n",
        "            padding_idx (int): 텐서 패딩을 위한 인덱스;\n",
        "                torch.nn.Embedding를 참고하세요\n",
        "            dropout_p (float): 드롭아웃으로 활성화 출력을 0으로 만들 확률\n",
        "        \"\"\"\n",
        "        super(SurnameGenerationModel, self).__init__()\n",
        "        \n",
        "        self.char_emb = nn.Embedding(num_embeddings=char_vocab_size,\n",
        "                                     embedding_dim=char_embedding_size,\n",
        "                                     padding_idx=padding_idx)\n",
        "\n",
        "        self.rnn = nn.GRU(input_size=char_embedding_size, \n",
        "                          hidden_size=rnn_hidden_size,\n",
        "                          batch_first=batch_first)\n",
        "        \n",
        "        self.fc = nn.Linear(in_features=rnn_hidden_size, \n",
        "                            out_features=char_vocab_size)\n",
        "        \n",
        "        self._dropout_p = dropout_p\n",
        "\n",
        "    def forward(self, x_in, apply_softmax=False):\n",
        "        \"\"\"모델의 정방향 계산\n",
        "        \n",
        "        매개변수:\n",
        "            x_in (torch.Tensor): 입력 데이터 텐서\n",
        "                x_in.shape는 (batch, input_dim)입니다.\n",
        "            apply_softmax (bool): 소프트맥스 활성화를 위한 플래그로 훈련시에는 False가 되어야 합니다.\n",
        "        반환값:\n",
        "            결과 텐서. tensor.shape는 (batch, char_vocab_size)입니다.\n",
        "        \"\"\"\n",
        "        x_embedded = self.char_emb(x_in)\n",
        "\n",
        "        y_out, _ = self.rnn(x_embedded)\n",
        "\n",
        "        batch_size, seq_size, feat_size = y_out.shape\n",
        "        y_out = y_out.contiguous().view(batch_size * seq_size, feat_size)\n",
        "\n",
        "        y_out = self.fc(F.dropout(y_out, p=self._dropout_p))\n",
        "                         \n",
        "        if apply_softmax:\n",
        "            y_out = F.softmax(y_out, dim=1)\n",
        "            \n",
        "        new_feat_size = y_out.shape[-1]\n",
        "        y_out = y_out.view(batch_size, seq_size, new_feat_size)\n",
        "            \n",
        "        return y_out"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXkGm58nVocj"
      },
      "source": [
        "### 7.3.5 모델2 : 조건이 있는 SurnameGenerationModel\n",
        "\n",
        "성씨 생성할때 국적을 고려\n",
        "\n",
        "은닉 상태 크기의 벡터로 국적을 임베딩하여 RNN의 초기 은닉 상태를 만든다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow3AzKPpV6aP"
      },
      "source": [
        "class SurnameGenerationModel(nn.Module):\n",
        "    def __init__(self, char_embedding_size, char_vocab_size, num_nationalities, rnn_hidden_size, \n",
        "                 batch_first=True, padding_idx=0, dropout_p=0.5):\n",
        "        \"\"\"\n",
        "        매개변수:\n",
        "            char_embedding_size (int): 문자 임베딩 크기\n",
        "            char_vocab_size (int): 임베딩될 문자 개수\n",
        "            rnn_hidden_size (int): RNN의 은닉 상태 크기\n",
        "            batch_first (bool): 0번째 차원이 배치인지 시퀀스인지 나타내는 플래그\n",
        "            padding_idx (int): 텐서 패딩을 위한 인덱스;\n",
        "                torch.nn.Embedding를 참고하세요\n",
        "            dropout_p (float): 드롭아웃으로 활성화 출력을 0으로 만들 확률\n",
        "        \"\"\"\n",
        "        super(SurnameGenerationModel, self).__init__()\n",
        "        \n",
        "        self.char_emb = nn.Embedding(num_embeddings=char_vocab_size,\n",
        "                                     embedding_dim=char_embedding_size,\n",
        "                                     padding_idx=padding_idx)\n",
        "\n",
        "        self.rnn = nn.GRU(input_size=char_embedding_size, \n",
        "                          hidden_size=rnn_hidden_size,\n",
        "                          batch_first=batch_first)\n",
        "        \n",
        "        self.fc = nn.Linear(in_features=rnn_hidden_size, \n",
        "                            out_features=char_vocab_size)\n",
        "        \n",
        "        self._dropout_p = dropout_p\n",
        "\n",
        "        ####################################\n",
        "\n",
        "        ## nation embedding\n",
        "        self.nation_emb = nn.Embedding(embedding_dim, rnn_hidden_size, num_embeddings=num_nationalities)\n",
        "\n",
        "        \n",
        "        ####################################\n",
        "\n",
        "    def forward(self, x_in, apply_softmax=False):\n",
        "        \"\"\"모델의 정방향 계산\n",
        "        \n",
        "        매개변수:\n",
        "            x_in (torch.Tensor): 입력 데이터 텐서\n",
        "                x_in.shape는 (batch, input_dim)입니다.\n",
        "            apply_softmax (bool): 소프트맥스 활성화를 위한 플래그로 훈련시에는 False가 되어야 합니다.\n",
        "        반환값:\n",
        "            결과 텐서. tensor.shape는 (batch, char_vocab_size)입니다.\n",
        "        \"\"\"\n",
        "        x_embedded = self.char_emb(x_in)\n",
        "        \n",
        "        ####################################\n",
        "\n",
        "        # hidden_size (num_layers * num_directions, batch_Size, rnn_hidden_size)\n",
        "        nationality_embedded = self.nation_emb(nationality_index).unsqueeze(0)\n",
        "\n",
        "        ####################################\n",
        "\n",
        "        y_out, _ = self.rnn(x_embedded)\n",
        "\n",
        "        batch_size, seq_size, feat_size = y_out.shape\n",
        "        y_out = y_out.contiguous().view(batch_size * seq_size, feat_size)\n",
        "\n",
        "        y_out = self.fc(F.dropout(y_out, p=self._dropout_p))\n",
        "                         \n",
        "        if apply_softmax:\n",
        "            y_out = F.softmax(y_out, dim=1)\n",
        "            \n",
        "        new_feat_size = y_out.shape[-1]\n",
        "        y_out = y_out.view(batch_size, seq_size, new_feat_size)\n",
        "            \n",
        "        return y_out"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LHhG5KhXTE3"
      },
      "source": [
        "### 7.3.6 모델 훈련과 결과\n",
        "\n",
        "**변경된 사항**\n",
        "1. 3차원 텐서를 2차원 텐서로 변환\n",
        "\n",
        "    3차원 텐서 (배치 차원, 시퀀스, 예측 벡터)\n",
        "2. 가변 길이 시퀀스를 위해 마스킹 인덱스 준비\n",
        "\n",
        "    마스킹된 인덱스에서는 손실 계산 안함\n",
        "\n",
        "\n",
        "1. 예측과 타깃을 손실 함수가 기대하는 크기 (예측은 2차원, 타깃은 1차원)으로 정규화\n",
        "2. 각 행은 하나의 샘플, 즉 시퀀스에 있는 하나의 타임 스텝을 나타냄"
      ]
    }
  ]
}