{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_Embedding.ipynb",
      "provenance": [],
      "mount_file_id": "1k5Sx9AdhMbyhBp2qA6KRs-GG7a7cAJ2J",
      "authorship_tag": "ABX9TyOemDq0biFFR5u8olT0ZRbe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junieberry/NLP-withPyTorch/blob/main/05_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWn3Q86hBJXA",
        "outputId": "040f95f7-d9b6-4eae-bcaa-951321cef3c2"
      },
      "source": [
        "# annoy 패키지를 설치합니다.\n",
        "!pip install annoy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting annoy\n",
            "  Downloading annoy-1.17.0.tar.gz (646 kB)\n",
            "\u001b[?25l\r\u001b[K     |▌                               | 10 kB 21.5 MB/s eta 0:00:01\r\u001b[K     |█                               | 20 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |██                              | 40 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 51 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███                             | 61 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 71 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |████                            | 81 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████                           | 102 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 112 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████                          | 122 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 133 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████                         | 143 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 153 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 163 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 174 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 184 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 194 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 204 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 215 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 225 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 235 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 245 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 256 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 266 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 276 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 286 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 296 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 307 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 317 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 327 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 337 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 348 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 358 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 368 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 378 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 389 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 399 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 409 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 419 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 430 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 440 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 450 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 460 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 471 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 481 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 491 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 501 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 512 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 522 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 532 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 542 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 552 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 563 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 573 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 583 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 593 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 604 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 614 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 624 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 634 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 646 kB 5.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.0-cp37-cp37m-linux_x86_64.whl size=391603 sha256=ac01381fbba6aa63421bf6e2957e9d92be49f4ff46f2a54d6e4d1570bd088896\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/e8/1e/7cc9ebbfa87a3b9f8ba79408d4d31831d67eea918b679a4c07\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLVdG19oBNHb"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from annoy import AnnoyIndex\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRzTUBpeI2q5"
      },
      "source": [
        "\n",
        "## 5.1 단어 임베딩을 배우는 이유\n",
        "\n",
        "**단어 벡터 표현을 만드는 전통적인 방법**\n",
        "\n",
        "1. 원-핫 표현\n",
        "\n",
        "  벡터의 길이는 어휘사전의 크기와 같고, 값은 1 혹은 0이다.\n",
        "\n",
        "2. 카운트 기반 표현\n",
        "\n",
        "  벡터의 길이는 어휘사전의 크기와 같지만 값은 단어의 빈도에 상응한다.\n",
        "\n",
        "  경험적으로 얻어진다.\n",
        "\n",
        "\n",
        "**밀집 벡터의 장점**\n",
        "\n",
        "1. 계산의 효율성\n",
        "2. 통계적 장점\n",
        "3. 고차원 입력은.. *차원의저주*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOHVUy81-34a"
      },
      "source": [
        "### 5.1.2 단어 임베딩 학습 방법\n",
        "\n",
        "**단어의 통계적, 언어적 속성을 감지하기 위한 보조 작업**\n",
        "\n",
        "1. 단어 시퀀스가 주어지면 다음 단어를 예측\n",
        "2. 앞 뒤 단어 시퀀스가 주어지면 가운데 단어를 예측 (CBOW)\n",
        "3. 단어가 주어지면 위치와 상관 없는 단어 예측 (Skipgram)\n",
        "\n",
        "e.g GloVe, CBOW (continuous Bag of Word), Skipgram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPCdjC8pAGHW"
      },
      "source": [
        "### 5.1.3 사전 훈련된 단어 임베딩\n",
        "\n",
        "**임베딩 로드**\n",
        "\n",
        "임베딩 포맷 = (임베딩 단어 / 임베딩 벡터 표현)\n",
        "\n",
        "`PreTrainedEmbedding` 클래스를 구현해보자!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7cWF_nkIU3X"
      },
      "source": [
        "class PreTrainedEmbedding(object):\n",
        "\n",
        "  def __init__(self, word_to_index, word_vectors):\n",
        "\n",
        "    self.word_to_index = word_to_index\n",
        "    self.word_vectors = word_vectors\n",
        "    ##\n",
        "    self.index_to_word = {v: k for k, v in self.word_to_index.items()}\n",
        "\n",
        "    ## 인덱스 만들기\n",
        "    self.index = AnnoyIndex(len(word_vectors[0]), metric=\"euclidean\")\n",
        "    for _, i in self.word_to_index.items():\n",
        "      self.index.add_item(i, self.word_vectors[i])\n",
        "    self.index.build(50) ## ????\n",
        "  \n",
        "\n",
        "  # 주어진 파일으로 PretrainedEmbedding 인스턴스 반환\n",
        "  ## embedding_file = 파일 위치\n",
        "  ## return == PretrainedEmbedding 인스턴스\n",
        "  @classmethod\n",
        "  def from_embeddings_file(cls, embedding_file):\n",
        "    \n",
        "    word_to_index = {}\n",
        "    word_vectors = []\n",
        "\n",
        "    with open(embedding_file) as fp:\n",
        "      for line in fp.readlines():\n",
        "        line = line.split(\" \")\n",
        "        word = line[0]\n",
        "        vec = np.array([float(x) for x in line[1:]])\n",
        "\n",
        "        word_to_index[word] = len(word_to_index)\n",
        "        word_vectors.append(vec)\n",
        "\n",
        "    return cls(word_to_index, word_vectors)\n",
        "  \n",
        "  # 주어진 단어의 임베딩 표현 반환\n",
        "  ## word = 단어\n",
        "  ## return == 임베딩 (numpy.ndarray)\n",
        "  def get_embedding(self, word):\n",
        "    return self.word_vectors[self.word_to_index[word]]\n",
        "  \n",
        "  # 주어진 벡터의 n개의 최근접 이웃 반환 (Annoy가 해줌)\n",
        "  ## vector = 주어진 벡터 (np.ndarray)\n",
        "  ## n = 반횐될 이웃의 개수 (int)\n",
        "  ## return == 주어진 벡터와 가까운 단어들\n",
        "  def get_closest_to_vector(self, vector, n=1):\n",
        "    nn_indices = self.index.get_nns_by_vector(vector, n)\n",
        "    return [self.index_to_word[neighbor] for neighbor in nn_indices]\n",
        "  \n",
        "  # 단어의 유추 관계\n",
        "  ## word1 : word2 = word2 : word4\n",
        "  ## word4 출력\n",
        "\n",
        "  def compute_and_print_analogy(self, word1, word2, word3):\n",
        "\n",
        "    vec1 = self.get_embedding(word1)\n",
        "    vec2 = self.get_embedding(word2)\n",
        "    vec3 = self.get_embedding(word3)\n",
        "\n",
        "    spatial_relationship = vec2 - vec1\n",
        "    vec4 =vec3 + spatial_relationship\n",
        "\n",
        "    closest_words = self.get_closest_to_vector(vec4, n=4)\n",
        "    existing_words = set([word1, word2, word3])\n",
        "    closest_words = [word for word in closest_words if word not in existing_words] \n",
        "\n",
        "    if len(closest_words) == 0:\n",
        "      print(\"계산된 벡터와 가장 가까운 이웃을 찾을 수 없습니다!\")\n",
        "      return\n",
        "        \n",
        "    for word4 in closest_words:\n",
        "      print(\"{} : {} :: {} : {}\".format(word1, word2, word3, word4))\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIufFOSuGowW",
        "outputId": "86121135-c028-451d-a5a8-bda758e2ce4a"
      },
      "source": [
        "# GloVe 데이터를 다운로드합니다.\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "!mkdir -p data/glove\n",
        "!mv glove.6B.100d.txt data/glove"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-22 10:43:58--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-09-22 10:43:58--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-09-22 10:43:59--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 40s  \n",
            "\n",
            "2021-09-22 10:46:39 (5.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttnXxCgmMJbV"
      },
      "source": [
        "embeddings = PreTrainedEmbedding.from_embeddings_file('data/glove/glove.6B.100d.txt')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGbArmhJMSuE",
        "outputId": "0cf78607-9bb1-47ff-8754-f2d7fe5b21bc"
      },
      "source": [
        "## 성별 명사와 대명사의 관계\n",
        "embeddings.compute_and_print_analogy('man', 'he', 'woman')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "man : he :: woman : she\n",
            "man : he :: woman : her\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk7bzuPvMhus",
        "outputId": "4536d93e-752c-4def-c383-b9e1be375abf"
      },
      "source": [
        "## 언어 규칙과 문화 편견\n",
        "embeddings.compute_and_print_analogy('man', 'doctor', 'woman')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "man : doctor :: woman : nurse\n",
            "man : doctor :: woman : physician\n",
            "man : doctor :: woman : doctors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fb8DxnRf8Jl"
      },
      "source": [
        "## 5.2 CBOW 임베딩 학습하기\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_JOKR0dAdVU"
      },
      "source": [
        "## 5.3 문서 분류에 사전 훈련된 임베딩을 사용한 전이 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwCiRfypBdnb"
      },
      "source": [
        "## 5.4 요약\n",
        "\n",
        "단어 임베딩의 편향 제거, 문맥 모델링, 다의성 등의 주제가 있다.\n",
        "\n"
      ]
    }
  ]
}