{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_Yelp_Classify.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOh76roO6vZ2kyZ5VmirEjZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junieberry/NLP-withPyTorch/blob/main/03_Yelp/03_Yelp_Classify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENdZD336Pkrq"
      },
      "source": [
        "공부한 코드 : https://github.com/rickiepark/nlp-with-pytorch/tree/main/chapter_3\n",
        "\n",
        "\n",
        "함께한 재생목록 : https://www.youtube.com/watch?v=L3Chu2A5TyU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkdKUBE20hPw",
        "outputId": "31c88321-7498-446b-e123-d3285b96d504"
      },
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.10.0+cu102)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchvision) (3.7.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7sV50kF0eVG"
      },
      "source": [
        "from argparse import Namespace\n",
        "from collections import Counter\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tqdm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zc5aiJHx0rR"
      },
      "source": [
        "## 3.6 레스토랑 리뷰 감성 분류하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_HSDRvZ0XeQ"
      },
      "source": [
        "### 3.6.2  파이토치 데이터셋 이해하기\n",
        "\n",
        "파이토치는 Dataset 클래스로 데이터셋을 추상화한다.\n",
        "\n",
        "Dataset은 `__getitem()`과 `__len()` 메서드를 구현한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0UDghdoxzCm"
      },
      "source": [
        "class ReviewDataset(Dataset):\n",
        "  ## review_df (pandas.DataFrame) == 데이터셋\n",
        "  ## vectorizer (ReviewVectorizer) == ReviewVectorizer 객체\n",
        "  def __init__(self, review_df, vectorizer):\n",
        "    self.review_df = review_df\n",
        "    self._vectorizer = vectorizer\n",
        "\n",
        "    self.train_df = self.review_df[self.review_df.split=='train']\n",
        "    self.train_size = len(self.train_df)\n",
        "\n",
        "    self.val_df = self.review_df[self.review_df.split=='val']\n",
        "    self.validation_size = len(self.val_df)\n",
        "\n",
        "    self.test_df = self.review_df[self.review_df.split=='test']\n",
        "    self.test_size = len(self.test_df)\n",
        "\n",
        "    self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
        "                          'val': (self.val_df, self.validation_size),\n",
        "                         'test': (self.test_df, self.test_size)}\n",
        "\n",
        "    self.set_split('train')\n",
        "  \n",
        "\n",
        "  ## classmethod는 static과 같다고 볼 수 있따..\n",
        "  @classmethod\n",
        "\n",
        "  ## 데이터셋 로드하고 새 RevewVectorizer을 만든당\n",
        "  ## review_csv (str) == 데이터셋 위치\n",
        "  ## return == ReviewDataset의 인스턴스\n",
        "  def load_dataset_and_make_vectorizer(cls, review_csv):\n",
        "    review_df = pd.read_csv(review_csv)\n",
        "    train_review_df = review_df[review_df.split=='train']\n",
        "    return cls(review_df,ReviewVectorizer.from_dataframe(train_review_df))\n",
        "\n",
        "  @classmethod\n",
        "  ## 데이터셋 로드하고 ReviewVectorizer 재사용\n",
        "  ## review_csv (str) == 데이터셋 위치\n",
        "  ## vectorizer_filepath (str) == ReviewVectorizer 객체의 저장 위치\n",
        "  ## return == ReviewDataset의 인스턴스\n",
        "\n",
        "  def load_Dataset_and_load_vectorizer(cls, review_csv, vectorizer_filepath):\n",
        "    review_df = pd.read_csv(review_csv)\n",
        "    vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
        "    return cls(review_df, vectorizer)\n",
        "  \n",
        "  @staticmethod\n",
        "  ## 파일에서 ReviewVectorizer 로드\n",
        "  ## vectorizer_filepath (str) == 직렬화된 ReviewVectorizer 객체의 위치\n",
        "  ## return == ReviewVectorizer의 인스턴스\n",
        "  def load_vectorizer_only(vectorizer_filepath):\n",
        "    with open(vectorizer_filepath) as fp:\n",
        "        return ReviewVectorizer.from_serializable(json.load(fp))\n",
        "  \n",
        "  ## ReviewVectorizer 객체를 json 형태로 저장\n",
        "  ## vectorizer_filepath (str) == ReviewVectorizer의 저장 위치\n",
        "  def save_vectorizer(self, vectorizer_filepath):\n",
        "    with open(vectorizer_filepath, \"w\") as fp:\n",
        "      json.dump(self._vectorizer.to_serializable(), fp)\n",
        "  \n",
        "  ## 백터 변환 객체 반환\n",
        "  def get_vectorizer(self):\n",
        "    return self._vectorizer\n",
        "  \n",
        "\n",
        "  ## 데이터 프레임에 있는 열을 사용해 분할 세트 선택\n",
        "  ## split (str) == \"train\", \"val\", \"test\" 중 하나\n",
        "  def set_split(self, split=\"train\"):\n",
        "    self._target_split = split\n",
        "    self._target_df, self._target_size = self._lookup_dict[split]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self._target_size\n",
        "\n",
        "  \n",
        "  ## 데이터셋의 진입 메서드\n",
        "  ## index (int) == 데이터 포인트 인덱스\n",
        "  ## return == x_data와 y_target으로 이루어진 딕셔너리\n",
        "  def __getitem__(self, index):\n",
        "    row = self._target_df.iloc[index]\n",
        "\n",
        "    review_vector = self._vectorizer.vectorize(row.review)\n",
        "    rating_index = self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
        "\n",
        "    return {'x_data' : review_vector, 'y_target':rating_index}\n",
        "  \n",
        "  ## 배치 크기를 받아 배치 개수를 반환\n",
        "  def get_num_batches(self, batch_size):\n",
        "    return len(self)//batch_size\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIKsRKkC0WRV"
      },
      "source": [
        "### 3.6.3 Vocabulary, Vectorizer, Dataloader\n",
        "\n",
        "토큰을 정수에 매핑하고, 이 매핑을 각 데이터 포인트에 적용해 백터 형태로 변환\n",
        "\n",
        "그 후 벡터로 변환한 데이터 포인트를 모델을 위해 미니배치로 모음\n",
        "\n",
        "Vocabulary = 정수-토큰 매핑\n",
        "\n",
        "Vocabulary = 텍스트 토큰과 클래스 레이블 정수로 매핑\n",
        "\n",
        "Vectorizer = 어휘 사전 캡츌화, 문자열 데이터를 수치 벡터로 변환\n",
        "\n",
        "DataLoader = 개별 벡터 데이터 포인트를 미니배치로"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6rkmAr19DlE"
      },
      "source": [
        "**Vocabulary**\n",
        "\n",
        "토큰을 정수로 매핑!\n",
        "\n",
        "- 토큰이 추가되면 자동으로 인덱스 증가\n",
        "- UNK 토큰 사용\n",
        "\n",
        "\n",
        "- `add_token` = vocabulary에 새로운 토큰 추가\n",
        "- `lookup_token` = 토큰에 해당하는 인덱스 추가\n",
        "- `lookup_index` = 특정 인덱스에 해당하는 토큰 추가\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecTOuDBPBeBd"
      },
      "source": [
        "class Vocabulary(object):\n",
        "\n",
        "  ##token_to_idx (dict): 기존 토큰-인덱스 매핑 딕셔너리\n",
        "  ## add_unk (bool): UNK 토큰을 추가할지 지정하는 플래그\n",
        "  ## unk_token (str): Vocabulary에 추가할 UNK 토큰\n",
        "  def __intit__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
        "\n",
        "    if token_to_idx is None:\n",
        "      self._token_to_idx = {}\n",
        "    \n",
        "    self._token_to_idx = token_to_idx\n",
        "\n",
        "    ## ???\n",
        "    self._token_to_idx = {idx:token\n",
        "                          for token, idx in self._token_to_idx.items()}\n",
        "    self._add_unk = add_unk\n",
        "    self._unk_token = unk_token\n",
        "\n",
        "    self.unk_index = -1\n",
        "    if add_unk:\n",
        "      self.unk_index = self.add_token(unk_token)\n",
        "  \n",
        "  ## 직렬화할 수 있는 딕셔너리를 반환\n",
        "  def to_serializable(self):\n",
        "    return { 'token_to_idx': self._token_to_idx,\n",
        "            'add_unk': self._add_unk,\n",
        "            'unk_token': self._unk_token\n",
        "    }\n",
        "  \n",
        "  @classmethod\n",
        "  ## 직렬화된 딕셔너리에서 Vocabulary 객체를 만든다.\n",
        "  def from_serializable(cls, contents):\n",
        "    return cls(**contents)\n",
        "  \n",
        "  ## 토큰 추가하고 매핑 딕셔너리를 업데이트\n",
        "  ## token (str) == Vocabulary에 추가할 토큰\n",
        "  ## return == 토큰의 index\n",
        "  def add_token(self, token):\n",
        "    if token in self._token_to_idx:\n",
        "      index = self._token_to_idx[token]\n",
        "    else:\n",
        "      index = len(self._token_to_idx)\n",
        "      self._token_to_idx[token] = index\n",
        "      self._idx_to_token[index] = token\n",
        "    return index\n",
        "\n",
        "\n",
        "  ## 토큰들을 추가하고 매핑 딕셔너리를 업데이트\n",
        "  ## tokens (list) == 문자열 토큰 리스트\n",
        "  ## return == 토큰들의 indices\n",
        "  def add_many(self, tokens):\n",
        "    return [self.add_token(token) for token in tokens]\n",
        "\n",
        "\n",
        "  ## 토큰에 대응하는 인덱스 추출, 토큰이 없으면 UNK 인덱스 반환\n",
        "  ## token (str) == 찾을 토큰\n",
        "  ## index (int) == 토큰에 해당하는 인덱스\n",
        "  def lookup_token(self, token):\n",
        "        \n",
        "        ## ??????\n",
        "        if self.unk_index >= 0:\n",
        "            return self._token_to_idx.get(token, self.unk_index)\n",
        "        else:\n",
        "            return self._token_to_idx[token]\n",
        "\n",
        "  ## 인덱스에 해당하는 토큰 반환\n",
        "  ## index (int) == 찾을 인덱스\n",
        "  ## return = 인덱스의 토큰\n",
        "  def lookup_index(self, index):\n",
        "    if index not in self._idx_to_token:\n",
        "      raise KeyError(\"Vocabulary에 인덱스(%d)가 없습니다.\" % index)\n",
        "    return self._idx_to_token[index]\n",
        "\n",
        "  def __str__(self):\n",
        "    return \"<Vocabulary(size=%d)>\" % len(self)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self._token_to_idx)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PTUobXTImX1"
      },
      "source": [
        "**Vectorizer**\n",
        "\n",
        "입력 데이터의 토큰을 순회하며 각 토큰을 정수로 바꿔 벡터로 만든다.\n",
        "\n",
        "이때 벡터들의 크기는 모두 같아야한다!\n",
        "\n",
        "\n",
        " `from_dataframe()` = Dataframe을 순회하며 Vectorizer 클래스 초기화\n",
        "  1. 데이터셋에 있는 모든 토큰의 빈도수 카운트\n",
        "  2. cutoff보다 빈도수가 높은 Vocabulary 객체만 남김\n",
        "\n",
        "`vectorize()` = Vectorizer 클래스의 핵심 기능 캡슐화\n",
        "\n",
        "  - 매개변수로 리뷰 문자열을 받고 리뷰의 벡터 표현을 반환 (원핫 벡터)\n",
        "\n",
        "  - 이때 벡터 표현은 희소 표현이며 BoW 방식이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PtAy59JKDpS"
      },
      "source": [
        "class ReviewVectorizer(object):\n",
        "\n",
        "  ## review_vocab (Vocabulary) == 단어를 정수에 매핑하는 Vocabulary\n",
        "  ## rating_vocab (Vocabulary) == 클래스 레이블을 정수에 매핑하는 Vocabulary\n",
        "  def __init__(self, review_vocab, rating_vocab):\n",
        "    self.review_vocab = review_vocab\n",
        "    self.rating_vocab = rating_vocab\n",
        "  \n",
        "\n",
        "  ## 매개변수로 받은 리뷰를 원핫 벡터로 변환시킴\n",
        "  ## review (str) == 리뷰\n",
        "  ## return = 변환된 원핫 벡터\n",
        "  def vectorize(self, review):\n",
        "    one_hot = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
        "\n",
        "    for token in review.split(\" \"):\n",
        "\n",
        "      ## string.punctuation == !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
        "      if token not in string.punctuation:\n",
        "        one_hot[self.review_vocab.lookup_token(token)] = 1\n",
        "    return one_hot\n",
        "  \n",
        "  @classmethod\n",
        "  ## 데이터셋 데이터프레임에서 Vectorizer 객체를 만든다\n",
        "  ## review_df (pandas.DataFrame) == 리뷰 데이터셋\n",
        "  ## cutoff (int) == 빈도 기반 필터링 설정값\n",
        "  ## return == ReviewVectorizer 객체\n",
        "  def from_dataframe(cls, review_df, cutoff=25):\n",
        "    review_vocab = Vocabulary(add_unk=True)\n",
        "    rating_vocab = Vocabulary(add_unk=False)\n",
        "\n",
        "    ## 점수 추가\n",
        "    for rating in sorted(set(review_df.rating)):\n",
        "      rating_vocab.add_token(rating)\n",
        "    \n",
        "    ## cutoff보다 많이 등장하는 단어 추가\n",
        "    word_couts = Counter()\n",
        "    for review in review_df.review:\n",
        "      for word in review.split(\" \"):\n",
        "        if word not in string.punctuation:\n",
        "          word_counts[word] += 1\n",
        "    \n",
        "    for word, count in word_counts.items():\n",
        "      if count > cutoff:\n",
        "        review_vocab.add_token(word)\n",
        "    \n",
        "    return cls(review_vocab, rating_vocab)\n",
        "\n",
        "    \"\"\" 직렬화된 딕셔너리에서 ReviewVectorizer 객체를 만듭니다\n",
        "      \n",
        "      매개변수:\n",
        "          contents (dict): 직렬화된 딕셔너리\n",
        "      반환값:\n",
        "           ReviewVectorizer 클래스 객체\n",
        "      \"\"\"\n",
        "  @classmethod\n",
        "  def from_serializable(cls, contents):\n",
        "      \n",
        "    review_vocab = Vocabulary.from_serializable(contents['review_vocab'])\n",
        "    rating_vocab =  Vocabulary.from_serializable(contents['rating_vocab'])\n",
        "\n",
        "    return cls(review_vocab=review_vocab, rating_vocab=rating_vocab)\n",
        "\n",
        "  def to_serializable(self):\n",
        "    \"\"\" 캐싱을 위해 직렬화된 딕셔너리를 만듭니다\n",
        "        \n",
        "    반환값:\n",
        "        contents (dict): 직렬화된 딕셔너리\n",
        "    \"\"\"\n",
        "    return {'review_vocab': self.review_vocab.to_serializable(), 'rating_vocab': self.rating_vocab.to_serializable()}"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sstau74CKrfu",
        "outputId": "b04861cd-fba3-4d33-a48a-cc2d0a19227d"
      },
      "source": [
        "print(string.punctuation)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G4hmARTMuXv"
      },
      "source": [
        "**DataLoader**\n",
        "\n",
        "미니 배치로 모아줌\n",
        "\n",
        "Dataset, batch_size 등을 매개변수로 받아 순회\n",
        "\n",
        "https://dojang.io/mod/page/view.php?id=2412"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy19m_g9M5-d"
      },
      "source": [
        "def generate_batches(dataset, batch_size, shuffle=True,\n",
        "                     drop_last=True, device=\"cpu\"):\n",
        "\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
        "                            shuffle=shuffle, drop_last=drop_last)\n",
        "\n",
        "\n",
        "    for data_dict in dataloader:\n",
        "        out_data_dict = {}\n",
        "        for name, tensor in data_dict.items():\n",
        "            out_data_dict[name] = data_dict[name].to(device)\n",
        "        yield out_data_dict"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1SJSbHYPcIv"
      },
      "source": [
        "### 3.6.4 퍼센트론 분류기\n",
        "\n"
      ]
    }
  ]
}